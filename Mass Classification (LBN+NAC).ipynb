{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3b0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Uniform\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from lbn_NAC import LBN\n",
    "\n",
    "PATH_DATASETS = \".\"\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 500 if AVAIL_GPUS else 64\n",
    "# BATCH_SIZE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "344551a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79c02cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aabe3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mass(x):\n",
    "    return torch.sqrt(x[...,0]**2 - x[...,1]**2 - x[...,2]**2 - x[...,3]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9de29ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_four_vectors(n, p_low=-100., p_high=100., m_low=0.1, m_high=50.):\n",
    "    \"\"\"\n",
    "    Creates a numpy array with shape ``n + (4,)`` describing four-vectors of particles whose\n",
    "    momentum components are uniformly distributed between *p_low* and *p_high*, and masses between\n",
    "    *m_low* and *m_high*.\n",
    "    \"\"\"\n",
    "    # create random four-vectors\n",
    "    if not isinstance(n, tuple):\n",
    "        n = (n,)\n",
    "    vecs = np.random.uniform(p_low, p_high, n + (4,)).astype(np.float32)\n",
    "\n",
    "    # the energy is also random and might be lower than the momentum,\n",
    "    # so draw uniformly distributed masses, and compute and insert the energy\n",
    "    m = np.abs(np.random.uniform(m_low, m_high, n))\n",
    "    p = np.sqrt(np.sum(vecs[..., 1:]**2, axis=-1))\n",
    "    E = (p**2 + m**2)**0.5\n",
    "    vecs[..., 0] = E\n",
    "\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbe21bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_mass(p):\n",
    "    m = mass(p)\n",
    "    g1 = torch.where(m >= 25.)[0]\n",
    "    g = torch.zeros(m.shape)\n",
    "    g[g1] = 1.0\n",
    "    return g.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74d33198",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = create_four_vectors(100)\n",
    "p = p.reshape(-1,1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00ef90da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-15 09:04:21.473510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-15 09:04:21.474000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-15 09:04:21.474179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-15 09:04:21.474481: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-15 09:04:21.475110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-15 09:04:21.475273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-15 09:04:21.475404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-15 09:04:23.708543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-15 09:04:23.708716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-15 09:04:23.708841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-15 09:04:23.708959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1709 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "lbn = LBN(10, boost_mode=LBN.PAIRS)\n",
    "lbn.build(p.shape, features=[\"E\", \"pt\", \"eta\", \"phi\", \"m\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9a5d998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ed5cd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 50])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbn(p).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c03b703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyData(Dataset):\n",
    "    def __init__(self, p_lbn, g, m):\n",
    "        self.p_lbn = p_lbn\n",
    "        self.g = g\n",
    "        self.m = m\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.p_lbn.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.p_lbn[idx,:], self.g[idx], self.m[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4751ca",
   "metadata": {},
   "source": [
    "## 1. LBN + DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81d107a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_with_LBN(pl.LightningModule):\n",
    "    def __init__(self, N=20000, hparams=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.N = N\n",
    "        \n",
    "        hidden_layer = hparams[\"hidden_layer\"]\n",
    "        hidden_depth = hparams[\"hidden_depth\"]\n",
    "        learning_rate = hparams[\"learning_rate\"]\n",
    "        batch_size = hparams[\"batch_size\"]\n",
    "        \n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.hidden_depth = hidden_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "#         self.max_lr = hparams[\"max_lr\"]\n",
    "        self.epochs = hparams[\"epochs\"]\n",
    "        \n",
    "        layers = [nn.Linear(50, hidden_layer), nn.ReLU(), nn.BatchNorm1d(hidden_layer)]\n",
    "        for i in range(hidden_depth):\n",
    "            layers.extend([\n",
    "                nn.Linear(hidden_layer, hidden_layer),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_layer)\n",
    "            ])\n",
    "        layers.append(nn.Linear(hidden_layer, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        hparams = {\n",
    "            \"hidden_layer\": hidden_layer,\n",
    "            \"hidden_depth\": hidden_depth,\n",
    "            \"batch_size\": batch_size,\n",
    "        }\n",
    "    \n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "        self.ds = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        p, g, _ = batch\n",
    "        m = self(p)\n",
    "        loss = F.binary_cross_entropy_with_logits(m, g)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        p, g, m_ans = batch\n",
    "        m = self(p)\n",
    "        loss = F.binary_cross_entropy_with_logits(m, g)\n",
    "        acc = self.accuracy(\n",
    "            torch.round(torch.sigmoid(m)),\n",
    "            g.int()\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            criterion = nn.MSELoss(reduction=\"mean\")\n",
    "            rmse = torch.sqrt(criterion(m + 25, m_ans) + 1e-6)\n",
    "        \n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', acc)\n",
    "        self.log('rmse', rmse)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        p = create_four_vectors(self.N)\n",
    "        m = mass(torch.tensor(p))\n",
    "        g = group_by_mass(torch.tensor(p))\n",
    "        \n",
    "        p = p.reshape(-1,1,4)\n",
    "        lbn = LBN(10, boost_mode=LBN.PAIRS)\n",
    "        lbn.build(p.shape, features=[\"E\", \"pt\", \"eta\", \"phi\", \"m\"])\n",
    "        p_lbn = lbn(p).numpy()\n",
    "        \n",
    "        self.W_hat = torch.tensor(lbn.W_hat.numpy())\n",
    "        self.M_hat = torch.tensor(lbn.M_hat.numpy())\n",
    "        self.ds = ToyData(p_lbn, g, m)\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        N_train = self.N // 10 * 7\n",
    "        N_val = self.N - N_train\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.ds_train, self.ds_val = random_split(self.ds, [N_train, N_val])\n",
    "        if stage == \"test\" or stage is None:\n",
    "            _, self.ds_test = random_split(self.ds, [N_train, N_val])\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.ds_train, batch_size=self.batch_size)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.ds_val, batch_size=self.batch_size)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.ds_test, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "668f4d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "hparams = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"epochs\": 200,\n",
    "    \"hidden_layer\": 64,\n",
    "    \"hidden_depth\": 3,\n",
    "}\n",
    "\n",
    "model = DNN_with_LBN(\n",
    "    hparams=hparams\n",
    ")\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project='LBN_Tutorial'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger=wandb_logger,\n",
    "    max_epochs=hparams[\"epochs\"],\n",
    "    gpus=AVAIL_GPUS,\n",
    "    enable_progress_bar=False,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=20, mode=\"min\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcbf73c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maxect\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/axect/LBN_Tutorial/runs/q92t3xcf\" target=\"_blank\">wandering-cherry-15</a></strong> to <a href=\"https://wandb.ai/axect/LBN_Tutorial\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | net      | Sequential | 16.3 K\n",
      "1 | accuracy | Accuracy   | 0     \n",
      "----------------------------------------\n",
      "16.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "16.3 K    Total params\n",
      "0.065     Total estimated model params size (MB)\n",
      "/home/xteca/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/xteca/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([500])) that is different to the input size (torch.Size([500, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Global seed set to 125\n",
      "/home/xteca/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/xteca/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:428: UserWarning: The number of training samples (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55a2568f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6605... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>rmse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▂▂▂▂▃▃▃▃▃▃▄▄▅▅▆▆▆▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>val_loss</td><td>██▇▇▇▇▇▇▇▇▇▆▆▅▅▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>150</td></tr><tr><td>rmse</td><td>17.58071</td></tr><tr><td>trainer/global_step</td><td>4227</td></tr><tr><td>val_acc</td><td>0.9215</td></tr><tr><td>val_loss</td><td>0.22758</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">wandering-cherry-15</strong>: <a href=\"https://wandb.ai/axect/LBN_Tutorial/runs/q92t3xcf\" target=\"_blank\">https://wandb.ai/axect/LBN_Tutorial/runs/q92t3xcf</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220415_090425-q92t3xcf/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91ba5390",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_data = list(iter(model.val_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccef85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = val_data[0][0]\n",
    "gs = val_data[0][1]\n",
    "ms = val_data[0][2]\n",
    "m_hats = model(ps)\n",
    "g_hats = torch.round(torch.sigmoid(m_hats))\n",
    "m_hats = m_hats + 25\n",
    "\n",
    "ps = ps.detach().numpy()\n",
    "gs = gs.detach().numpy()\n",
    "ms = ms.detach().numpy()\n",
    "m_hats = m_hats.detach().numpy()\n",
    "g_hats = g_hats.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46e8ca85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m</th>\n",
       "      <th>m_hat</th>\n",
       "      <th>g</th>\n",
       "      <th>g_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.636046</td>\n",
       "      <td>6.354189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42.882839</td>\n",
       "      <td>33.568871</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.482122</td>\n",
       "      <td>27.915802</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.922801</td>\n",
       "      <td>13.494654</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.219436</td>\n",
       "      <td>14.974447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>45.320107</td>\n",
       "      <td>36.525131</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>48.277569</td>\n",
       "      <td>35.954433</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>22.755516</td>\n",
       "      <td>25.992397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>35.825844</td>\n",
       "      <td>32.032360</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>43.889057</td>\n",
       "      <td>30.324829</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             m      m_hat    g  g_hat\n",
       "0     6.636046   6.354189  0.0    0.0\n",
       "1    42.882839  33.568871  1.0    1.0\n",
       "2    31.482122  27.915802  1.0    1.0\n",
       "3     0.922801  13.494654  0.0    0.0\n",
       "4     9.219436  14.974447  0.0    0.0\n",
       "..         ...        ...  ...    ...\n",
       "495  45.320107  36.525131  1.0    1.0\n",
       "496  48.277569  35.954433  1.0    1.0\n",
       "497  22.755516  25.992397  0.0    1.0\n",
       "498  35.825844  32.032360  1.0    1.0\n",
       "499  43.889057  30.324829  1.0    1.0\n",
       "\n",
       "[500 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg = pd.DataFrame({\n",
    "    \"m\": ms,\n",
    "    \"m_hat\": m_hats[:,0],\n",
    "    \"g\": gs[:,0],\n",
    "    \"g_hat\": g_hats[:,0]\n",
    "})\n",
    "\n",
    "dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4610dd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29027395629882813"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.sum((ms - m_hats[:,0])**2)) / len(ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49920082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
